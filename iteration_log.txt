# AlphaZero Training Log - Iteration 1 Regression Problem

## Setup
- **Model**: 30 residual blocks, ~10M parameters
- **Iteration 0**: Trained random weights on 2k random self-play games
  - **Result vs Random**: 74.6% score rate (61.6% wins, 25.9% draws) - STRONG improvement
- **Goal**: Train Iteration 1 to beat Iteration 0

---

## Problem Discovery
Iteration 1 consistently fails to improve over Iteration 0, and sometimes regresses (gets worse).

---

## Experiments Conducted

### Initial Discovery - Data Ratio Matters
**Test**: Different mixing ratios of iteration-0 vs random data

| Config | Result | Notes |
|--------|--------|-------|
| 2k iter-0 + 2k random, lr=0.0001 | 46.4% score | New model WORSE than old |
| 2k iter-0 + 1k random, lr=0.0001 | 59.8% score (56 games) | First success! |
| 2k iter-0 + 1k random, lr=0.0001 | 51.8% score (112 games) | Regressed to mean, barely better |

**Key Finding**: 67% iteration-0 data + 33% random data seemed to help, but only marginally (51.8% ≈ coin flip).

---

### Learning Rate Experiments
**Test**: Does higher LR help? (All using 2k iter-0 + 1k random, 4 epochs)

| Learning Rate | Score Rate | New Wins | Old Wins | Draws |
|---------------|------------|----------|----------|-------|
| 0.0001 (baseline) | 46.4% | 39.3% | 35.7% | 25.0% |
| 0.0005 (5x) | 37.5% | 26.8% | 51.8% | 21.4% |
| 0.001 (10x) | 35.7% | 23.2% | 51.8% | 25.0% |

**Key Finding**: Higher learning rates make model progressively WORSE. Opposite of expected behavior.

---

## Root Cause Analysis

### Hypothesis 1: Learning Rate Too Low
- Original AlphaZero used lr=0.2 (2000x higher than 0.0001)
- **Status**: REJECTED - increasing LR made things worse

### Hypothesis 2: Strategy Collapse / Policy Collapse
- Iteration-0 learned strategies that beat random play specifically
- These strategies may not be generally good chess
- Training on iteration-0 data teaches "play like iteration-0" not "play better"
- Higher LR learns these narrow patterns more strongly → worse performance
- **Status**: LIKELY - supported by worsening performance with higher LR

### Hypothesis 3: Insufficient Data Diversity
- Only 2k games from iteration-0 may not be diverse enough
- Model overfits to iteration-0's habits with higher LR
- Random data (1k games) provides some diversity but not enough
- **Status**: POSSIBLE - needs testing with more data

---

## Key Insights

1. **Iteration 0→1 is fundamentally different from Random→0**:
   - Random self-play: Very diverse, exploratory
   - Iteration-0 self-play: Less diverse, model has developed preferences/habits

2. **The validation loss paradox**:
   - Validation loss goes UP during training
   - But model performance (when tested) roughly stays same or gets slightly worse
   - Suggests model learning patterns that don't generalize to actual play

3. **Data mixing helps but doesn't solve the problem**:
   - 2k iter-0 + 1k random is better than 2k iter-0 + 2k random
   - But still only achieves ~52% (barely better than random)

---

## Proposed Next Steps

### Option 1: Generate More Iteration-0 Data ⭐ RECOMMENDED
- Generate 2k-4k MORE games from iteration-0
- Train on 4k-5k iter-0 + 1k random with lr=0.001
- **Rationale**: More data diversity might allow higher LR to work

### Option 2: Generate New Random Data
- Generate fresh 2k random self-play games (different from original)
- Train iteration-0 on NEW random data with lr=0.001
- **Rationale**: Tests if iteration-0 can learn from fresh exploration

### Option 3: Reduce Iteration-0 Data Proportion
- Use 500-1k iter-0 + 2k random (80% random, 20% iter-0)
- **Rationale**: Minimize learning iteration-0's bad habits

### Option 4: Architectural/Training Changes
- Reset BatchNorm statistics when loading pretrained weights
- Use learning rate warmup or schedule
- Increase training epochs (15-20 instead of 4)
- Try Adam optimizer instead of SGD

---

## Implementation Notes
- Using fixed eval seed for reproducible 56-game tests (faster iteration)
- Gradient clipping at max_norm=10 already in place
- Batch size: 1024, ~150k training examples per iteration
- Early stopping: patience=3, min_improvement=0.25%

---

## References
- AlphaZero paper: lr=0.2 with decay schedule
- Strategy collapse documented in backgammon and Dota 2 (OpenAI)
- Leela Chess Zero: Kept 500k-1M games in replay buffer across many iterations