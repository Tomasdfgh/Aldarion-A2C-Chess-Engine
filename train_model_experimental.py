#!/usr/bin/env python3
"""
Neural Network Training Script for Aldarion Chess Engine

This script trains the ChessNet model on self-play data generated by parallel_training_data.py.
Data format: List of (board_fen, history_fens, move_probabilities_dict, game_outcome) tuples
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import argparse
import chess
from datetime import datetime
import matplotlib.pyplot as plt

# Import existing modules
import model as md
import board_reader as br

class ChessTrainingDataset(Dataset):
    """
    PyTorch Dataset for chess training data with fixed 4,672-move vocabulary
    """
    def __init__(self, data_files):
        
        all_training_data = []
        for data_file in data_files:
            if not os.path.exists(data_file):
                print(f"Error: Could not find {data_file}")
                continue

            try:
                with open(data_file, 'rb') as f:
                    data = pickle.load(f)
                all_training_data.extend(data)
            except Exception as e:
                print(f"Error loading {data_file}: {e}")
                continue
        
        self.training_data = all_training_data
        print(f"Dataset initialized with {len(self.training_data)} training examples")
    
    def __len__(self):
        return len(self.training_data)
    
    def __getitem__(self, idx):
        board_fen, history_fens, move_probs, game_outcome = self.training_data[idx]
        
        game_history = []
        for fen in history_fens:
            game_history.append(chess.Board(fen))
        
        current_board = chess.Board(board_fen)
        board_tensor = br.board_to_full_alphazero_input(current_board, game_history)

        policy_vector = torch.zeros((8, 8, 73), dtype=torch.float32)
        for move, prob in move_probs.items():
            try:
                r, c, pl = br.uci_to_policy_index(str(move), current_board.turn)
                policy_vector[r, c, pl] = float(prob)
            except:
                continue
        policy_vector = policy_vector.reshape(-1)
        
        s = policy_vector.sum()
        if s > 0:
            policy_vector /= s
        
        legal_mask = br.create_legal_move_mask(chess.Board(board_fen)).flatten()
        if not legal_mask.any():
            return None
        value_target = torch.tensor([game_outcome], dtype=torch.float32)
        
        return board_tensor.float(), policy_vector, legal_mask, value_target

def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)
    
    return torch.utils.data.dataloader.default_collate(batch)


def compute_loss(model_output, targets, legal_masks):
    """
    Compute AlphaZero loss with proper legal move masking.
    Heads up that L2 Regularization is not done here but instead done
    through the optimizer
    """

    policy_logits, value_pred = model_output
    target_policy, target_value = targets
    

    masked_logits = policy_logits.clone()
    masked_logits[~legal_masks.bool()] = -1000.0
    log_probs = F.log_softmax(masked_logits, dim=1)
    
    target_sum = target_policy.sum(dim=1, keepdim=True)
    normalized_target = target_policy / (target_sum + 1e-8)
    
    # Policy Loss: - pi * log (p)
    policy_loss = -(normalized_target * log_probs).sum(dim=1).mean()
    
    # Value loss: (v - t) ** 2
    value_loss = nn.MSELoss()(value_pred.squeeze(), target_value.squeeze())
    
    # Total Loss: (v - t) ** 2 - pi * log(p)
    total_loss = policy_loss +  value_loss
    
    return {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'value_loss': value_loss
    }


def train_epoch(model, dataloader, optimizer, device, epoch_num):
    """
    Train model for one epoch
    """
    model.train()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    for batch_idx, (board_tensors, target_policies, legal_masks, target_values) in enumerate(dataloader):

        if len(board_tensors) == 0:
            continue
            
        board_tensors = board_tensors.to(device)
        target_policies = target_policies.to(device)
        legal_masks = legal_masks.to(device)
        target_values = target_values.to(device)
        
        optimizer.zero_grad()
        policy_logits, value_pred = model(board_tensors)
        
        losses = compute_loss(
            (policy_logits, value_pred), 
            (target_policies, target_values),
            legal_masks
        )
        
        losses['total_loss'].backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
        
        optimizer.step()
        
        total_loss += losses['total_loss'].item()
        total_policy_loss += losses['policy_loss'].item()
        total_value_loss += losses['value_loss'].item()
        num_batches += 1
        
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch_num}, Batch {batch_idx}/{len(dataloader)}: "
                  f"Loss={losses['total_loss'].item():.4f}, "
                  f"Policy={losses['policy_loss'].item():.4f}, "
                  f"Value={losses['value_loss'].item():.4f}")
    
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics


def validate_model(model, dataloader, device):
    """
    Validate model on held-out data
    """
    model.eval()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():

        for board_tensors, target_policies, legal_masks, target_values in dataloader:

            if len(board_tensors) == 0:
                continue
                
            board_tensors = board_tensors.to(device)
            target_policies = target_policies.to(device)
            legal_masks = legal_masks.to(device)
            target_values = target_values.to(device)

            policy_logits, value_pred = model(board_tensors)
            
            losses = compute_loss(
                (policy_logits, value_pred), 
                (target_policies, target_values),
                legal_masks
            )
            
            total_loss += losses['total_loss'].item()
            total_policy_loss += losses['policy_loss'].item()
            total_value_loss += losses['value_loss'].item()
            num_batches += 1
    
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics

def create_alphazero_lr_scheduler(optimizer, total_epochs):
    """
    Adaptation of AlphaZero learning rate scheduler. They did a bit differently than me
    """
    def lr_lambda(epoch):
        decay_epoch1 = int(0.4 * total_epochs)
        decay_epoch2 = int(0.6 * total_epochs)
        
        if epoch < decay_epoch1:
            return 1.0
        elif epoch < decay_epoch2:
            return 0.1
        else:
            return 0.01
    
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def plot_training_metrics(train_metrics_history, val_metrics_history, save_path=None):
    epochs = range(1, len(train_metrics_history) + 1)
    
    _, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Total loss
    axes[0].plot(epochs, [m['avg_total_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[0].plot(epochs, [m['avg_total_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[0].set_title('Total Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)
    
    # Policy loss
    axes[1].plot(epochs, [m['avg_policy_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[1].plot(epochs, [m['avg_policy_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[1].set_title('Policy Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)
    
    # Value loss
    axes[2].plot(epochs, [m['avg_value_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[2].plot(epochs, [m['avg_value_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[2].set_title('Value Loss')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training curves saved to {save_path}")
    
    plt.show()


def main():
    parser = argparse.ArgumentParser(
        description='Train ChessNet on self-play data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    parser.add_argument('--data', type=str, nargs='+', required=True,
                        help='Path(s) to training data pickle file(s)')
    parser.add_argument('--epochs', type=int, default=10,
                        help='Number of training epochs (default: 10)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for training (default: 32)')
    parser.add_argument('--lr', type=float, default=0.2,
                        help='Learning rate (default: 0.2 for SGD, 0.001 for Adam)')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='L2 regularization weight decay (default: 1e-4)')
    parser.add_argument('--optimizer', type=str, default='sgd', choices=['sgd', 'adam'],
                        help='Optimizer type: sgd (AlphaZero paper) or adam (default: sgd)')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='SGD momentum (default: 0.9, AlphaZero paper value)')
    parser.add_argument('--lr_schedule', type=str, default='alphazero', choices=['alphazero', 'step', 'none'],
                        help='Learning rate schedule: alphazero, step, or none (default: alphazero)')
    parser.add_argument('--validation_data', type=str, nargs='+', default=None,
                        help='Path(s) to validation data pickle file(s) (optional)')
    parser.add_argument('--model_path', type=str, default='model_weights/model_weights.pth',
                        help='Path to initial model weights (default: model_weights/model_weights.pth)')
    parser.add_argument('--output', type=str, default=None,
                        help='Output directory for training plots (default: training_results/)')
    
    args = parser.parse_args()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("="*70)
    print("Initial Training Setup")
    print("="*70)
    print('\n')
    print(f"Using device: {device}")
    print(f"Optimizer: {args.optimizer.upper()}")
    if args.optimizer == 'sgd':
        print(f"Momentum: {args.momentum}")
    print(f"Learning rate: {args.lr}")
    print(f"LR Schedule: {args.lr_schedule}")
    print(f"Validation: {'Separate dataset' if args.validation_data else 'None'}")
    
    # Create datasets
    full_dataset = ChessTrainingDataset(args.data)
    
    if args.validation_data:
        train_dataset = full_dataset
        val_dataset = ChessTrainingDataset(args.validation_data)
        print(f"Train set: {len(train_dataset)}, Validation set: {len(val_dataset)}")
    else:
        # Split training data 90/10 for train/validation
        val_size = int(len(full_dataset) * 0.1)
        train_size = len(full_dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        print(f"Train set: {train_size}, Validation set: {val_size} (split from training data)")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=True if device.type == 'cuda' else False,
        collate_fn=collate_fn
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True if device.type == 'cuda' else False,
            collate_fn=collate_fn
        )
    
    # Initialize model
    model = md.ChessNet()
    model = model.to(device)
    
    # Load pretrained weights if available
    if os.path.exists(args.model_path):
        print(f"\nModel weights from {args.model_path}")
        try:
            state_dict = torch.load(args.model_path, map_location=device, weights_only=True)
            model.load_state_dict(state_dict, strict=False)
            print("Pretrained weights loaded successfully")
        except Exception as e:
            print(f"Could not load pretrained weights: {e}, so starting with random weights")
    else:
        print("No pretrained weights found. Starting with random initialization")
    
    # Setup optimizer and learning rate scheduler
    if args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
        print(f"Using SGD optimizer (AlphaZero paper): lr={args.lr}, momentum={args.momentum}")
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        print(f"Using Adam optimizer: lr={args.lr}")
    
    # Setup learning rate scheduler
    if args.lr_schedule == 'alphazero':
        scheduler = create_alphazero_lr_scheduler(optimizer, args.epochs)
        print(f"Using AlphaZero learning rate schedule (decay at epochs {int(0.4 * args.epochs)} and {int(0.6 * args.epochs)})")
    elif args.lr_schedule == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        print("Using step LR schedule (decay every 10 epochs)")
    else:
        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 1.0)
        print("Using constant learning rate (no schedule)")
    
    # Training loop
    print(f"\nStarting training for {args.epochs} epochs...")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    train_metrics_history = []
    val_metrics_history = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 3
    min_improvement = 0.0025  # 0.25% minimum improvement threshold

    print('\n')
    print("="*70)
    print("Training Begins")
    print("="*70)
    print('\n')
    
    for epoch in range(1, args.epochs + 1):
        print(f"Epoch {epoch}/{args.epochs}")
        print("-" * 50)
        
        # Training
        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)
        train_metrics_history.append(train_metrics)
        
        print(f"Training - Loss: {train_metrics['avg_total_loss']:.4f}, "
              f"Policy: {train_metrics['avg_policy_loss']:.4f}, "
              f"Value: {train_metrics['avg_value_loss']:.4f}")
        
        # Validation
        if val_loader:
            val_metrics = validate_model(model, val_loader, device)
            val_metrics_history.append(val_metrics)
            
            print(f"Validation - Loss: {val_metrics['avg_total_loss']:.4f}, "
                  f"Policy: {val_metrics['avg_policy_loss']:.4f}, "
                  f"Value: {val_metrics['avg_value_loss']:.4f}\n")
            
            # Early Stopping with relative improvement threshold
            current_val_loss = val_metrics['avg_total_loss']
            
            if best_val_loss == float('inf'):
                # First epoch - set baseline
                best_val_loss = current_val_loss
                patience_counter = 0
                print(f"Baseline validation loss: {current_val_loss:.4f}")
            else:
                improvement = (best_val_loss - current_val_loss) / best_val_loss
                
                if improvement >= min_improvement:
                    best_val_loss = current_val_loss
                    patience_counter = 0
                    print(f"Validation loss improved by {improvement*100:.2f}%")
                else:
                    patience_counter += 1
                    if improvement > 0:
                        print(f"Validation loss improved by only {improvement*100:.2f}% (< {min_improvement*100:.2f}% threshold)")
                    else:
                        print(f"Validation loss increased by {abs(improvement)*100:.2f}%")
                    
                    if patience_counter >= patience:
                        print(f"Early stopping triggered: no improvement â‰¥{min_improvement*100:.2f}% for {patience} epochs")
                        break
        
        scheduler.step()
    
    # Set output directory for both model and plots
    if args.output is None:
        output_dir = "training_results"
        model_dir = "model_weights"
    else:
        output_dir = args.output
        model_dir = args.output
    
    # Save final model
    os.makedirs(model_dir, exist_ok=True)
    final_path = os.path.join(model_dir, "model_weights_final.pth")
    torch.save(model.state_dict(), final_path)
    print(f"\nFinal model saved to {final_path}")
    
    # Plot training curves
    if len(train_metrics_history) > 1:
        plot_dir = output_dir
        os.makedirs(plot_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"training_curves_{timestamp}.png"
        plot_path = os.path.join(plot_dir, plot_filename)
        plot_training_metrics(train_metrics_history, val_metrics_history, plot_path)
    
    # Training summary
    print("\n" + "="*70)
    print("TRAINING COMPLETE")
    print("="*70)
    print(f"Final training loss: {train_metrics_history[-1]['avg_total_loss']:.4f}")
    if val_metrics_history:
        print(f"Final validation loss: {val_metrics_history[-1]['avg_total_loss']:.4f}")
    print(f"Total epochs: {args.epochs}")
    print(f"Final model: {final_path}")
    return final_path


if __name__ == "__main__":
    main()