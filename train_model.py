#!/usr/bin/env python3
"""
Neural Network Training Script for Aldarion Chess Engine

This script trains the ChessNet model on self-play data generated by parallel_training_data.py.
Data format: List of (board_fen, history_fens, move_probabilities_dict, game_outcome) tuples

Architecture:
- Loads training data from pickle files
- Converts FEN strings to 119-channel input tensors
- Converts move probability dictionaries to policy vectors  
- Trains using AlphaZero loss function (policy + value)
- Supports multiple data files and tensorboard logging
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import argparse
import numpy as np
import chess
from datetime import datetime
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Any
import traceback

# Import existing modules
import model as md
import board_reader as br


class ChessTrainingDataset(Dataset):
    """
    PyTorch Dataset for chess training data with fixed 4,672-move vocabulary
    
    Data format: List of (board_fen, history_fens, move_probabilities, game_outcome) tuples
    - board_fen: FEN string of the current position
    - history_fens: List of 8 FEN strings (current + 7 previous positions)
    - move_probabilities: Dict mapping chess.Move -> float probability  
    - game_outcome: Float value (-1 to 1, from perspective of player to move)
    """
    
    def __init__(self, data_files: List[str]):
        self.num_moves = 4672
        
        # Load training data from pickle files
        all_training_data = []
        for data_file in data_files:
            if not os.path.exists(data_file):
                print(f"Error: Could not find {data_file}")
                continue
                
            print(f"Loading training data from {data_file}...")
            try:
                with open(data_file, 'rb') as f:
                    data = pickle.load(f)
                all_training_data.extend(data)
                print(f"  Loaded {len(data)} examples from {data_file}")
            except Exception as e:
                print(f"  Error loading {data_file}: {e}")
                continue
        
        self.training_data = all_training_data
        print(f"Dataset initialized with {len(self.training_data)} training examples")
    
    def __len__(self):
        return len(self.training_data)
    
    def __getitem__(self, idx):
        board_fen, history_fens, move_probs, game_outcome = self.training_data[idx]
        
        # Convert board history list to chess.Board objects for board_reader
        game_history = []
        for fen in history_fens:
            game_history.append(chess.Board(fen))
        
        # Convert current board with history to 119-channel input tensor
        current_board = chess.Board(board_fen)
        board_tensor = br.board_to_full_alphazero_input(current_board, game_history)
        
        # Convert move probabilities to 4,672-dimensional policy vector
        policy_vector = torch.zeros(4672, dtype=torch.float32).reshape(8, 8, 73)
        for move, prob in move_probs.items():
            try:
                r, c, pl = br.uci_to_policy_index(str(move))
                policy_vector[r, c, pl] = float(prob)
            except:
                continue
        policy_vector = policy_vector.reshape(-1)
        
        # Defensive normalization in case move_probs aren't perfectly normalized
        s = policy_vector.sum()
        if s > 0:
            policy_vector /= s
        
        # Create legal move mask for this position
        board = chess.Board(board_fen)
        legal_mask = br.create_legal_move_mask(board).flatten()  # Flatten to 4672, bool type
        
        #Check to skip data point that is a terminal state
        if not legal_mask.any():
            return None
        
        # Game outcome as value target
        value_target = torch.tensor([game_outcome], dtype=torch.float32)
        
        return board_tensor.float(), policy_vector, legal_mask, value_target


def collate_fn(batch):
    """Custom collate function to handle None values (skipped samples)"""
    # Filter out None values
    batch = [item for item in batch if item is not None]
    
    if len(batch) == 0:
        # Return empty batch
        return torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)
    
    # Use default collate for non-None items
    return torch.utils.data.dataloader.default_collate(batch)




def compute_loss(model_output, targets, legal_masks, policy_weight=1.0, value_weight=1.0):
    """
    Compute AlphaZero loss with proper legal move masking
    
    Args:
        model_output: Tuple of (policy_logits, value_pred) - logits are raw scores
        targets: Tuple of (target_policy, target_value)
        legal_masks: Tensor of legal move masks for each position in batch
        policy_weight: Weight for policy loss
        value_weight: Weight for value loss
    
    Returns:
        Dictionary with total loss and component losses
    """
    policy_logits, value_pred = model_output
    target_policy, target_value = targets
    
    # Simple approach: just mask with reasonable negative value and use standard cross-entropy
    masked_logits = policy_logits.clone()
    masked_logits[~legal_masks.bool()] = -1000.0  # Large enough to be ignored
    
    # Use KL divergence which handles probability distributions well
    log_probs = F.log_softmax(masked_logits, dim=1)
    
    # Ensure target is valid probability distribution
    target_sum = target_policy.sum(dim=1, keepdim=True)
    normalized_target = target_policy / (target_sum + 1e-8)
    
    # KL divergence: sum(target * (log(target) - log(pred)))
    policy_loss = -(normalized_target * log_probs).sum(dim=1).mean()
    
    # Value loss: MSE between game outcome and predicted value
    value_loss = nn.MSELoss()(value_pred.squeeze(), target_value.squeeze())
    
    # Combined loss
    total_loss = policy_weight * policy_loss + value_weight * value_loss
    
    return {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'value_loss': value_loss
    }


def train_epoch(model, dataloader, optimizer, device, epoch_num):
    """
    Train model for one epoch
    
    Returns:
        Dictionary with training metrics
    """
    model.train()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    for batch_idx, (board_tensors, target_policies, legal_masks, target_values) in enumerate(dataloader):
        # Skip empty batches (from filtered terminal positions)
        if len(board_tensors) == 0:
            continue
            
        # Move to device
        board_tensors = board_tensors.to(device)
        target_policies = target_policies.to(device)
        legal_masks = legal_masks.to(device)
        target_values = target_values.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        policy_logits, value_pred = model(board_tensors)  # Raw logits now
        
        # Compute loss with legal move masking
        losses = compute_loss(
            (policy_logits, value_pred), 
            (target_policies, target_values),
            legal_masks
        )
        
        # Backward pass
        losses['total_loss'].backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        
        # Accumulate metrics
        total_loss += losses['total_loss'].item()
        total_policy_loss += losses['policy_loss'].item()
        total_value_loss += losses['value_loss'].item()
        num_batches += 1
        
        # Print progress
        if batch_idx % 100 == 0:
            print(f"  Epoch {epoch_num}, Batch {batch_idx}/{len(dataloader)}: "
                  f"Loss={losses['total_loss'].item():.4f}, "
                  f"Policy={losses['policy_loss'].item():.4f}, "
                  f"Value={losses['value_loss'].item():.4f}")
    
    # Calculate average metrics
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics


def validate_model(model, dataloader, device):
    """
    Validate model on held-out data
    
    Returns:
        Dictionary with validation metrics
    """
    model.eval()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for board_tensors, target_policies, legal_masks, target_values in dataloader:
            # Skip empty batches (from filtered terminal positions)
            if len(board_tensors) == 0:
                continue
                
            # Move to device
            board_tensors = board_tensors.to(device)
            target_policies = target_policies.to(device)
            legal_masks = legal_masks.to(device)
            target_values = target_values.to(device)
            
            # Forward pass
            policy_logits, value_pred = model(board_tensors)
            
            # Compute loss with legal move masking
            losses = compute_loss(
                (policy_logits, value_pred), 
                (target_policies, target_values),
                legal_masks
            )
            
            # Accumulate metrics
            total_loss += losses['total_loss'].item()
            total_policy_loss += losses['policy_loss'].item()
            total_value_loss += losses['value_loss'].item()
            num_batches += 1
    
    # Calculate average metrics
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics

def create_alphazero_lr_scheduler(optimizer, total_epochs):
    """
    Create AlphaZero learning rate scheduler
    Paper schedule: LR starts at base_lr, decays by 10x at steps 400k and 600k
    We'll adapt this to epoch-based training with reasonable decay points
    """
    def lr_lambda(epoch):
        # Adapt AlphaZero's step-based schedule to epoch-based
        # Original: decay at 400k (66.7%), 600k (100%) steps  
        # Adapted: decay at 60%, 80% of total epochs
        decay_epoch1 = int(0.6 * total_epochs)  # First decay at 60%
        decay_epoch2 = int(0.8 * total_epochs)  # Second decay at 80%
        
        if epoch < decay_epoch1:
            return 1.0      # Full learning rate
        elif epoch < decay_epoch2:
            return 0.1      # 10x decay
        else:
            return 0.01     # 100x decay
    
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def plot_training_metrics(train_metrics_history, val_metrics_history, save_path=None):
    """Plot training curves"""
    epochs = range(1, len(train_metrics_history) + 1)
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Total loss
    axes[0].plot(epochs, [m['avg_total_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[0].plot(epochs, [m['avg_total_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[0].set_title('Total Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)
    
    # Policy loss
    axes[1].plot(epochs, [m['avg_policy_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[1].plot(epochs, [m['avg_policy_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[1].set_title('Policy Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)
    
    # Value loss
    axes[2].plot(epochs, [m['avg_value_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[2].plot(epochs, [m['avg_value_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[2].set_title('Value Loss')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training curves saved to {save_path}")
    
    plt.show()


def main():
    """Main training function"""
    parser = argparse.ArgumentParser(
        description='Train ChessNet on self-play data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Train on single data file
  python3 train_model.py --data training_data/parallel_training_data_20241201.pkl
  
  # Train on multiple files with custom parameters (SGD default)
  python3 train_model.py --data file1.pkl file2.pkl --epochs 20 --lr 0.2
  
  # Use Adam optimizer instead of SGD
  python3 train_model.py --data file1.pkl --optimizer adam --lr 0.001
  
  
  # Training with validation split
  python3 train_model.py --data large_dataset.pkl --validation_split 0.1
  
        """
    )
    
    parser.add_argument('--data', type=str, nargs='+', required=True,
                        help='Path(s) to training data pickle file(s)')
    parser.add_argument('--epochs', type=int, default=10,
                        help='Number of training epochs (default: 10)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for training (default: 32)')
    parser.add_argument('--lr', type=float, default=0.2,
                        help='Learning rate (default: 0.2 for SGD, 0.001 for Adam)')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='L2 regularization weight decay (default: 1e-4)')
    parser.add_argument('--optimizer', type=str, default='sgd', choices=['sgd', 'adam'],
                        help='Optimizer type: sgd (AlphaZero paper) or adam (default: sgd)')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='SGD momentum (default: 0.9, AlphaZero paper value)')
    parser.add_argument('--lr_schedule', type=str, default='alphazero', choices=['alphazero', 'step', 'none'],
                        help='Learning rate schedule: alphazero, step, or none (default: alphazero)')
    parser.add_argument('--validation_split', type=float, default=0.1,
                        help='Fraction of data for validation (default: 0.1)')
    parser.add_argument('--model_path', type=str, default='model_weights/model_weights.pth',
                        help='Path to initial model weights (default: model_weights/model_weights.pth)')
    parser.add_argument('--output', type=str, default=None,
                        help='Output directory for training plots (default: training_results/)')
    
    args = parser.parse_args()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    print(f"Optimizer: {args.optimizer.upper()}")
    if args.optimizer == 'sgd':
        print(f"Momentum: {args.momentum}")
    print(f"Learning rate: {args.lr}")
    print(f"LR Schedule: {args.lr_schedule}")
    
    # Create dataset (loads training data internally)
    print("Loading training data...")
    dataset = ChessTrainingDataset(args.data)
    
    # Split into train/validation
    if args.validation_split > 0:
        val_size = int(len(dataset) * args.validation_split)
        train_size = len(dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        print(f"Train set: {train_size}, Validation set: {val_size}")
    else:
        train_dataset = dataset
        val_dataset = None
        print(f"Train set: {len(train_dataset)} (no validation)")
    
    # Create data loaders with custom collate function
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=True if device.type == 'cuda' else False,
        collate_fn=collate_fn
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True if device.type == 'cuda' else False,
            collate_fn=collate_fn
        )
    
    # Initialize model
    print("Initializing model...")
    model = md.ChessNet()
    model = model.to(device)
    
    # Load pretrained weights if available
    if os.path.exists(args.model_path):
        print(f"Loading pretrained weights from {args.model_path}")
        try:
            state_dict = torch.load(args.model_path, map_location=device, weights_only=True)
            model.load_state_dict(state_dict, strict=False)
            print("Pretrained weights loaded successfully")
        except Exception as e:
            print(f"Could not load pretrained weights: {e}")
            print("Starting with random initialization")
    else:
        print("No pretrained weights found. Starting with random initialization")
    
    # Setup optimizer and learning rate scheduler
    if args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), 
                             lr=args.lr, 
                             momentum=args.momentum, 
                             weight_decay=args.weight_decay)
        print(f"Using SGD optimizer (AlphaZero paper): lr={args.lr}, momentum={args.momentum}")
    else:  # adam
        optimizer = optim.Adam(model.parameters(), 
                              lr=args.lr, 
                              weight_decay=args.weight_decay)
        print(f"Using Adam optimizer: lr={args.lr}")
    
    # Setup learning rate scheduler
    if args.lr_schedule == 'alphazero':
        scheduler = create_alphazero_lr_scheduler(optimizer, args.epochs)
        decay_epoch1 = int(0.6 * args.epochs)
        decay_epoch2 = int(0.8 * args.epochs)
        print(f"Using AlphaZero learning rate schedule (decay at epochs {decay_epoch1} and {decay_epoch2})")
    elif args.lr_schedule == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        print("Using step LR schedule (decay every 10 epochs)")
    else:  # none
        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 1.0)
        print("Using constant learning rate (no schedule)")
    
    
    # Training loop
    print(f"\nStarting training for {args.epochs} epochs...")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    train_metrics_history = []
    val_metrics_history = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 10
    
    for epoch in range(1, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        print("-" * 50)
        
        # Training
        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)
        train_metrics_history.append(train_metrics)
        
        print(f"Training - Loss: {train_metrics['avg_total_loss']:.4f}, "
              f"Policy: {train_metrics['avg_policy_loss']:.4f}, "
              f"Value: {train_metrics['avg_value_loss']:.4f}")
        
        # Validation
        if val_loader:
            val_metrics = validate_model(model, val_loader, device)
            val_metrics_history.append(val_metrics)
            
            print(f"Validation - Loss: {val_metrics['avg_total_loss']:.4f}, "
                  f"Policy: {val_metrics['avg_policy_loss']:.4f}, "
                  f"Value: {val_metrics['avg_value_loss']:.4f}")
            
            # Early stopping check
            if val_metrics['avg_total_loss'] < best_val_loss:
                best_val_loss = val_metrics['avg_total_loss']
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping triggered after {patience} epochs without improvement")
                    break
        
        # Learning rate scheduling
        scheduler.step()
    
    # Final model save
    os.makedirs("model_weights", exist_ok=True)
    final_path = os.path.join("model_weights", "model_weights_final.pth")
    torch.save(model.state_dict(), final_path)
    print(f"\nFinal model saved to {final_path}")
    
    # Plot training curves
    if len(train_metrics_history) > 1:
        # Set output directory for plots
        if args.output is None:
            plot_dir = "training_results"
        else:
            plot_dir = args.output
        
        os.makedirs(plot_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"training_curves_{timestamp}.png"
        plot_path = os.path.join(plot_dir, plot_filename)
        plot_training_metrics(train_metrics_history, val_metrics_history, plot_path)
    
    # Training summary
    print("\n" + "="*60)
    print("TRAINING COMPLETE")
    print("="*60)
    print(f"Final training loss: {train_metrics_history[-1]['avg_total_loss']:.4f}")
    if val_metrics_history:
        print(f"Final validation loss: {val_metrics_history[-1]['avg_total_loss']:.4f}")
    print(f"Total epochs: {args.epochs}")
    print(f"Final model: {final_path}")
    
    return final_path


if __name__ == "__main__":
    main()