#!/usr/bin/env python3
"""
Neural Network Training Script for Aldarion Chess Engine

This script trains the ChessNet model on self-play data generated by parallel_training_data.py.
Data format: List of (board_fen, move_probabilities_dict, game_outcome) tuples

Architecture:
- Loads training data from pickle files
- Converts FEN strings to 119-channel input tensors
- Converts move probability dictionaries to policy vectors  
- Trains using AlphaZero loss function (policy + value)
- Supports multiple data files, model checkpointing, and tensorboard logging
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import argparse
import numpy as np
import chess
from datetime import datetime
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Any
import traceback

# Import existing modules
import model as md
import board_reader as br


class ChessTrainingDataset(Dataset):
    """
    PyTorch Dataset for chess training data with fixed 4,672-move vocabulary
    
    Data format: List of (board_fen, move_probabilities, game_outcome) tuples
    - board_fen: FEN string of the position
    - move_probabilities: Dict mapping chess.Move -> float probability  
    - game_outcome: Float value (-1 to 1, from perspective of player to move)
    """
    
    def __init__(self, training_data: List[Tuple[str, Dict, float]]):
        self.training_data = training_data
        self.num_moves = 4672  # AlphaZero 8×8×73 = 4,672 moves
        
        print(f"Dataset initialized with {len(training_data)} training examples")
        print(f"Using AlphaZero 8×8×73 policy head: {self.num_moves} moves")
    
    def __len__(self):
        return len(self.training_data)
    
    def __getitem__(self, idx):
        board_fen, move_probs, game_outcome = self.training_data[idx]
        
        # Convert board to 119-channel input tensor
        board_tensor = br.board_to_full_alphazero_input(board_fen)
        
        # Convert move probabilities to 4,672-dimensional policy vector
        policy_vector = torch.zeros(4672, dtype=torch.float32)
        for move, prob in move_probs.items():
            try:
                r, c, pl = br.uci_to_policy_index(str(move))
                idx = (r * 73) + pl + (c * 73 * 8)  # 8×8×73 flattening
                policy_vector[idx] = float(prob)
            except:
                # Skip invalid moves
                continue
        
        # Create legal move mask for this position
        board = chess.Board(board_fen)
        legal_mask = br.create_legal_move_mask(board).flatten()  # Flatten to 4672, bool type
        
        # Safeguard: Skip positions with no legal moves (shouldn't happen in training data)
        if not legal_mask.any():
            # This is a terminal position (mate/stalemate) - skip it
            # Return None to signal this sample should be skipped
            return None
        
        # Game outcome as value target
        value_target = torch.tensor([game_outcome], dtype=torch.float32)
        
        return board_tensor.float(), policy_vector, legal_mask, value_target


def collate_fn(batch):
    """Custom collate function to handle None values (skipped samples)"""
    # Filter out None values
    batch = [item for item in batch if item is not None]
    
    if len(batch) == 0:
        # Return empty batch
        return torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)
    
    # Use default collate for non-None items
    return torch.utils.data.dataloader.default_collate(batch)


def load_training_data(data_files: List[str]) -> List[Tuple[str, Dict, float]]:
    """
    Load and combine training data from multiple pickle files
    
    Args:
        data_files: List of pickle file paths (automatically checks training_data/ directory)
    
    Returns:
        Combined training data list
    """
    all_training_data = []
    
    for data_file in data_files:
        # Check if file exists as-is, otherwise look in training_data directory
        if os.path.exists(data_file):
            file_path = data_file
        elif os.path.exists(os.path.join("training_data", data_file)):
            file_path = os.path.join("training_data", data_file)
        elif os.path.exists(os.path.join("training_data", os.path.basename(data_file))):
            file_path = os.path.join("training_data", os.path.basename(data_file))
        else:
            print(f"  Error: Could not find {data_file} in current directory or training_data/")
            continue
            
        print(f"Loading training data from {file_path}...")
        
        try:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
            
            if isinstance(data, list) and len(data) > 0:
                # Check data format
                sample = data[0]
                if len(sample) == 3 and isinstance(sample[0], str):
                    all_training_data.extend(data)
                    print(f"  Loaded {len(data)} examples from {file_path}")
                else:
                    print(f"  Warning: Unexpected data format in {data_file}")
            else:
                print(f"  Warning: Empty or invalid data in {data_file}")
                
        except Exception as e:
            print(f"  Error loading {file_path}: {e}")
            continue
    
    print(f"Total training examples loaded: {len(all_training_data)}")
    return all_training_data


def compute_loss(model_output, targets, legal_masks, policy_weight=1.0, value_weight=1.0):
    """
    Compute AlphaZero loss with proper legal move masking
    
    Args:
        model_output: Tuple of (policy_logits, value_pred) - logits are raw scores
        targets: Tuple of (target_policy, target_value)
        legal_masks: Tensor of legal move masks for each position in batch
        policy_weight: Weight for policy loss
        value_weight: Weight for value loss
    
    Returns:
        Dictionary with total loss and component losses
    """
    policy_logits, value_pred = model_output
    target_policy, target_value = targets
    
    # Apply legal move masking to logits
    masked_logits = policy_logits.clone()
    masked_logits[legal_masks == False] = -1e3  # Mask illegal moves (smaller value)
    
    # Apply softmax to get probabilities
    policy_probs = F.softmax(masked_logits, dim=1)
    
    # Cross-entropy loss: -sum(target * log(prediction))
    # Add small epsilon to prevent log(0)
    epsilon = 1e-8
    policy_loss = -torch.sum(target_policy * torch.log(policy_probs + epsilon), dim=1).mean()
    
    # Value loss: MSE between game outcome and predicted value
    value_loss = nn.MSELoss()(value_pred.squeeze(), target_value.squeeze())
    
    # Combined loss
    total_loss = policy_weight * policy_loss + value_weight * value_loss
    
    return {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'value_loss': value_loss
    }


def train_epoch(model, dataloader, optimizer, device, epoch_num):
    """
    Train model for one epoch
    
    Returns:
        Dictionary with training metrics
    """
    model.train()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    for batch_idx, (board_tensors, target_policies, legal_masks, target_values) in enumerate(dataloader):
        # Skip empty batches (from filtered terminal positions)
        if len(board_tensors) == 0:
            continue
            
        # Move to device
        board_tensors = board_tensors.to(device)
        target_policies = target_policies.to(device)
        legal_masks = legal_masks.to(device)
        target_values = target_values.to(device)
        
        # Forward pass
        optimizer.zero_grad()
        policy_logits, value_pred = model(board_tensors)  # Raw logits now
        
        # Compute loss with legal move masking
        losses = compute_loss(
            (policy_logits, value_pred), 
            (target_policies, target_values),
            legal_masks
        )
        
        # Backward pass
        losses['total_loss'].backward()
        optimizer.step()
        
        # Accumulate metrics
        total_loss += losses['total_loss'].item()
        total_policy_loss += losses['policy_loss'].item()
        total_value_loss += losses['value_loss'].item()
        num_batches += 1
        
        # Print progress
        if batch_idx % 100 == 0:
            print(f"  Epoch {epoch_num}, Batch {batch_idx}/{len(dataloader)}: "
                  f"Loss={losses['total_loss'].item():.4f}, "
                  f"Policy={losses['policy_loss'].item():.4f}, "
                  f"Value={losses['value_loss'].item():.4f}")
    
    # Calculate average metrics
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics


def validate_model(model, dataloader, device):
    """
    Validate model on held-out data
    
    Returns:
        Dictionary with validation metrics
    """
    model.eval()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for board_tensors, target_policies, legal_masks, target_values in dataloader:
            # Skip empty batches (from filtered terminal positions)
            if len(board_tensors) == 0:
                continue
                
            # Move to device
            board_tensors = board_tensors.to(device)
            target_policies = target_policies.to(device)
            legal_masks = legal_masks.to(device)
            target_values = target_values.to(device)
            
            # Forward pass
            policy_logits, value_pred = model(board_tensors)
            
            # Compute loss with legal move masking
            losses = compute_loss(
                (policy_logits, value_pred), 
                (target_policies, target_values),
                legal_masks
            )
            
            # Accumulate metrics
            total_loss += losses['total_loss'].item()
            total_policy_loss += losses['policy_loss'].item()
            total_value_loss += losses['value_loss'].item()
            num_batches += 1
    
    # Calculate average metrics
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics


def save_model_checkpoint(model, optimizer, epoch, metrics, checkpoint_path):
    """Save model checkpoint with training state"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics,
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')
    }
    
    torch.save(checkpoint, checkpoint_path)
    print(f"Model checkpoint saved to {checkpoint_path}")


def plot_training_metrics(train_metrics_history, val_metrics_history, save_path=None):
    """Plot training curves"""
    epochs = range(1, len(train_metrics_history) + 1)
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Total loss
    axes[0].plot(epochs, [m['avg_total_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[0].plot(epochs, [m['avg_total_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[0].set_title('Total Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)
    
    # Policy loss
    axes[1].plot(epochs, [m['avg_policy_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[1].plot(epochs, [m['avg_policy_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[1].set_title('Policy Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)
    
    # Value loss
    axes[2].plot(epochs, [m['avg_value_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[2].plot(epochs, [m['avg_value_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[2].set_title('Value Loss')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training curves saved to {save_path}")
    
    plt.show()


def main():
    """Main training function"""
    parser = argparse.ArgumentParser(
        description='Train ChessNet on self-play data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Train on single data file (automatically finds in training_data/)
  python3 train_model.py --data parallel_training_data_20241201.pkl
  
  # Train on multiple files with custom parameters
  python3 train_model.py --data file1.pkl file2.pkl --epochs 20 --lr 0.001
  
  # Resume training from checkpoint
  python3 train_model.py --data latest.pkl --resume model_checkpoint_epoch_10.pth
  
  # Training with validation split
  python3 train_model.py --data large_dataset.pkl --validation_split 0.1
  
  # Files are automatically searched in training_data/ directory if not found in current dir
        """
    )
    
    parser.add_argument('--data', type=str, nargs='+', required=True,
                        help='Path(s) to training data pickle file(s)')
    parser.add_argument('--epochs', type=int, default=10,
                        help='Number of training epochs (default: 10)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for training (default: 32)')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='Learning rate (default: 0.001)')
    parser.add_argument('--weight_decay', type=float, default=1e-3,
                        help='L2 regularization weight decay (default: 1e-3)')
    parser.add_argument('--validation_split', type=float, default=0.1,
                        help='Fraction of data for validation (default: 0.1)')
    parser.add_argument('--resume', type=str, default=None,
                        help='Resume training from checkpoint')
    parser.add_argument('--model_path', type=str, default='model_weights/model_weights.pth',
                        help='Path to initial model weights (default: model_weights/model_weights.pth)')
    parser.add_argument('--output_dir', type=str, default='.',
                        help='Output directory for checkpoints and plots (default: current dir)')
    
    args = parser.parse_args()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # Load training data
    print("Loading training data...")
    training_data = load_training_data(args.data)
    
    if len(training_data) == 0:
        print("No training data loaded. Exiting.")
        sys.exit(1)
    
    # Create dataset
    dataset = ChessTrainingDataset(training_data)
    
    # Split into train/validation
    if args.validation_split > 0:
        val_size = int(len(dataset) * args.validation_split)
        train_size = len(dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        print(f"Train set: {train_size}, Validation set: {val_size}")
    else:
        train_dataset = dataset
        val_dataset = None
        print(f"Train set: {len(train_dataset)} (no validation)")
    
    # Create data loaders with custom collate function
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=True if device.type == 'cuda' else False,
        collate_fn=collate_fn
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True if device.type == 'cuda' else False,
            collate_fn=collate_fn
        )
    
    # Initialize model with fixed 4,672-move policy head
    print("Initializing model...")
    model = md.ChessNet()
    
    # Fixed AlphaZero policy head (8×8×73 = 4,672)
    model.linear3 = nn.Linear(2 * 8 * 8, 4672)
    model = model.to(device)
    print(f"Policy head size: 4,672 moves (8×8×73)")
    
    # Load pretrained weights if available
    start_epoch = 0
    if args.resume:
        print(f"Resuming from checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        start_epoch = checkpoint['epoch']
        print(f"Resumed from epoch {start_epoch}")
    elif os.path.exists(args.model_path):
        print(f"Loading pretrained weights from {args.model_path}")
        try:
            state_dict = torch.load(args.model_path, map_location=device, weights_only=True)
            # Handle potential size mismatch in policy head
            if 'linear3.weight' in state_dict and state_dict['linear3.weight'].shape[0] != 4672:
                print(f"Policy head size mismatch. Expected 4672, got {state_dict['linear3.weight'].shape[0]}")
                print("Removing policy head from pretrained weights (will be randomly initialized)")
                del state_dict['linear3.weight']
                del state_dict['linear3.bias']
            model.load_state_dict(state_dict, strict=False)
            print("Pretrained weights loaded successfully")
        except Exception as e:
            print(f"Could not load pretrained weights: {e}")
            print("Starting with random initialization")
    else:
        print("No pretrained weights found. Starting with random initialization")
    
    # Setup optimizer and learning rate scheduler
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
    
    # Resume optimizer state if checkpoint provided
    if args.resume and 'optimizer_state_dict' in checkpoint:
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("Optimizer state resumed")
        except Exception as e:
            print(f"Could not resume optimizer state: {e}")
    
    # Training loop
    print(f"\nStarting training for {args.epochs} epochs...")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    train_metrics_history = []
    val_metrics_history = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 5  # Early stopping patience
    
    for epoch in range(start_epoch + 1, start_epoch + args.epochs + 1):
        print(f"\nEpoch {epoch}/{start_epoch + args.epochs}")
        print("-" * 50)
        
        # Training
        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)
        train_metrics_history.append(train_metrics)
        
        print(f"Training - Loss: {train_metrics['avg_total_loss']:.4f}, "
              f"Policy: {train_metrics['avg_policy_loss']:.4f}, "
              f"Value: {train_metrics['avg_value_loss']:.4f}")
        
        # Validation
        if val_loader:
            val_metrics = validate_model(model, val_loader, device)
            val_metrics_history.append(val_metrics)
            
            print(f"Validation - Loss: {val_metrics['avg_total_loss']:.4f}, "
                  f"Policy: {val_metrics['avg_policy_loss']:.4f}, "
                  f"Value: {val_metrics['avg_value_loss']:.4f}")
            
            # Early stopping check
            if val_metrics['avg_total_loss'] < best_val_loss:
                best_val_loss = val_metrics['avg_total_loss']
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"Early stopping triggered after {patience} epochs without improvement")
                    break
        
        # Learning rate scheduling
        scheduler.step()
    
    # Final model save
    os.makedirs("model_weights", exist_ok=True)
    final_path = os.path.join("model_weights", "model_weights_final.pth")
    torch.save(model.state_dict(), final_path)
    print(f"\nFinal model saved to {final_path}")
    
    # Plot training curves
    if len(train_metrics_history) > 1:
        os.makedirs("training_results", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"training_curves_{timestamp}.png"
        plot_path = os.path.join("training_results", plot_filename)
        plot_training_metrics(train_metrics_history, val_metrics_history, plot_path)
    
    # Training summary
    print("\n" + "="*60)
    print("TRAINING COMPLETE")
    print("="*60)
    print(f"Final training loss: {train_metrics_history[-1]['avg_total_loss']:.4f}")
    if val_metrics_history:
        print(f"Final validation loss: {val_metrics_history[-1]['avg_total_loss']:.4f}")
    print(f"Total epochs: {args.epochs}")
    print(f"Final model: {final_path}")
    
    return final_path


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nTraining interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"Training failed with error: {e}")
        traceback.print_exc()
        sys.exit(1)